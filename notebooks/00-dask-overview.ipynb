{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4db5d14d-ee57-4791-9ab7-58cb2ff2cc3b",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/NCAR/dask-tutorial/main/images/NCAR-contemp-logo-blue.png\"\n",
    "     width=\"750px\"\n",
    "     alt=\"NCAR logo\"\n",
    "     style=\"vertical-align:middle;margin:30px 0px\"/>\n",
    "\n",
    "\n",
    "# Dask Overview\n",
    "\n",
    "**ESDS dask tutorial | 06 February, 2023**  \n",
    "\n",
    "Brian Vanderwende and Negin Sobhani  \n",
    "Computational & Information Systems Lab (CISL)  \n",
    "[vanderwb@ucar.edu](mailto:vanderwb@ucar.edu) and [negins@ucar.edu](negins@ucar.edu)\n",
    "\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6211a0-3762-41a2-8a45-6b19ce32f658",
   "metadata": {},
   "source": [
    "**In this tutorial, you learn:**\n",
    "\n",
    "* Dask basics and components of Dask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e012c-213f-43ad-997c-1f895241826e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Complex data structures enable data science in Python. For example:\n",
    "* [NumPy arrays](https://numpy.org/doc/stable/)\n",
    "* [Pandas series and dataframes](https://pandas.pydata.org/)\n",
    "* [XArray arrays](https://docs.xarray.dev/)\n",
    "\n",
    "*But datasets are getting larger all of the time! What if my data science is too big to fit into memory, or takes too long to complete an analysis?*\n",
    "\n",
    "## Introducing Dask\n",
    "### What is Dask?\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/latest/_images/dask_horizontal.svg\"\n",
    "     width=\"500px\"\n",
    "     alt=\"NCAR logo\"\n",
    "     style=\"vertical-align:middle;margin:30px 0px\"/>\n",
    "\n",
    "* Dask is an open-source Python library for parallel and distributed computing that scales the existing Python ecosystem.\n",
    "\n",
    "* Dask was developed to scale Python packages such as Numpy, Pandas, and Xarray to multi-core machines and distributed clusters when datasets exceed memory. Dask can scale up to full laptop capacity, thousand-node HPC clusters, and on the cloud.\n",
    "\n",
    "* Dask increases the size of possible work from *fits-in-memory* to *fits-on-disk* (sometimes doing it faster) via distributed parallelism. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" markdown=\"1\">\n",
    "\n",
    "<b>NOTE:</b> **Dask should only be used when necessary as it incurs overhead.**\n",
    "<ul>\n",
    "    Avoid Dask if you can easily:\n",
    "    <li> Speed up your code with use of compiled routines in libraries like NumPy</li>\n",
    "    <li> Profile and optimize your serial code to minimize bottlenecks</li>\n",
    "    <li> Read in a subset of data to gain the insight you need</li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/NCAR/dask-tutorial/main/images/dask_twitter.png\"\n",
    "     width=\"500px\"/>\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b446ce-24a6-48f6-a197-a5d1b97fb0db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dask is composed of two main parts\n",
    "\n",
    "#### 1.  Dask Collections\n",
    "\n",
    "A Dask *collection* is the fundamental thing we wish to parallelize. \n",
    "Dask features different levels of collection types:\n",
    "\n",
    "* ##### High-level collections \n",
    "Dask provides high-level Array, Bag, and DataFrame collections that mimic NumPy, lists, and pandas but can operate in parallel on datasets that donâ€™t fit into memory.\n",
    "\n",
    "    Most of the time, you will probably use one of the following *high-level* (big)data structures:\n",
    "\n",
    "| Collection | Serial | Dask |\n",
    "|-|-|-|\n",
    "| Arrays | numpy.array | dask.array.from_array |\n",
    "| Dataframes | pandas.read_csv | dask.dataframe.read_csv |\n",
    "| Unstructured | [1,2,3] | dask.bag.from_sequence([1,2,3]) |\n",
    "\n",
    "* ##### Low-level collections\n",
    "Dask also feature two low-level collection types - `delayed` and `futures`. These collections give user finer control to build custom parallel and distributed computations. \n",
    "    * **delayed** - run any arbitrary Python function using Dask task parallelism (think looped function calls)\n",
    "    * **futures** - similar to delayed but allows for concurrency on the client (think backgrounded processes)\n",
    "\n",
    "\n",
    "\n",
    "![Dask Collections](https://tutorial.dask.org/_images/high_vs_low_level_coll_analogy.png)\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Dynamic Task Scheduling\n",
    "When a computational task is submitted, the Dask distributed scheduler sends it off to a Dask cluster. We can basically think of the Dask scheduler as our task orchestrator. \n",
    "\n",
    "A Dask cluster consist of the following : \n",
    "\n",
    "* **scheduler** : A scheduler creates and manages task graphs and distributes tasks to workers.\n",
    "\n",
    "* **workers** : A worker is typically a separate Python process on either the local host or a remote machine. A Dask cluster usually consists of many workers. Basically, a worker is a Python interpretor which will perform work on a subset of our dataset.  \n",
    "\n",
    "* **client** - A client is a local object that points to the scheduler (often local but not always). \n",
    "\n",
    "\n",
    "![Dask Distributed Cluster](https://tutorial.dask.org/_images/distributed-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dd7f2e-fd55-4954-a599-247f55bc9210",
   "metadata": {},
   "source": [
    "### Why Dask?\n",
    "\n",
    "#### Familiar Interface\n",
    "\n",
    "In geosciences, many researchers often use Python libraries such as Xarray, Numpy, Pandas, and Scikit-Learn to analyze their simulations and observations. However, many atmospheric and oceanographic datasets consist of multi-dimensional arrays of numerical data, such as temperature sampled on a regular latitude, longitude, depth, and time grid. When researchers want to apply their analyses to larger datasets, they find that their developed tools are not scalable beyond a single machine. \n",
    "\n",
    "Dask collections such as Dask Array, Dask DataFrames provide decent NumPy and Pandas compatible APIs. This means Dask provides ways to parallelize Pandas, Xarray, and Numpy workflows with minimal code rewriting. \n",
    "\n",
    "#### Flexibility\n",
    "Dask provides several tools that help with data analysis on large datasets. For example, you can easily wrap your function in `dask.delayed` decorator to make it run in parallel. \n",
    "\n",
    "Dask also supports easily interfacing with popular HPC resource managers and job queueing system such as PBS, SLURM, and SGE. \n",
    "\n",
    "\n",
    "#### Scale up and scale down\n",
    "Dask scales well from single machine (laptop) to HPC clusters. \n",
    "This ease of transition between single machine to moderate clusters makes it easy for users to prototype their workflows on their local machines and seamlessy transition to a cluster when needed.  \n",
    "\n",
    "\n",
    "#### Responsiveness\n",
    "Dask provides rapid feedback and interactive dashboard to keep users informed on how the computation is progressing. This helps users identify and resolve potential issues earlier. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd1ccf7-eda1-4e40-bce5-02fd3cc95e2d",
   "metadata": {},
   "source": [
    "## How to follow this Tutorial:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbea0a3-6f90-41a1-9827-8f60771e67af",
   "metadata": {},
   "source": [
    "## Useful Links\n",
    "\n",
    "*  Reference\n",
    "    *  [Docs](https://dask.org/)\n",
    "    *  [Examples](https://examples.dask.org/)\n",
    "    *  [Code](https://github.com/dask/dask/)\n",
    "    *  [Blog](https://blog.dask.org/)\n",
    "*  Ask for help\n",
    "    *   [`dask`](http://stackoverflow.com/questions/tagged/dask) tag on Stack Overflow, for usage questions\n",
    "    *   [github issues](https://github.com/dask/dask/issues/new) for bug reports and feature requests\n",
    "    *   [discourse forum](https://dask.discourse.group/) for general, non-bug, questions and discussion\n",
    "    *   Attend a live tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2dfa9c-c276-4470-bd39-8827e08c4964",
   "metadata": {},
   "source": [
    "---\n",
    "## Addendum: Using JupyterLab on HPC systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae54c21-6ade-489d-96b8-506b429d2593",
   "metadata": {},
   "source": [
    "Ideally, you have access to JupyterHub, which provides a web portal to notebooks, terminals, and Dask Distributed dashboards. If not, you will need to create SSH tunnels to forward the port for Jupyter *and, if desired, the Dask dashboard.*  \n",
    "\n",
    "**Remote System**\n",
    "```\n",
    "conda activate my-dask-env\n",
    "jupyter lab --no-browser [--port 8888]\n",
    "```\n",
    "**Local System**\n",
    "```\n",
    "ssh -N -L 8888:localhost:8888 remote.hpc.system.edu\n",
    "```\n",
    "Then, you would navigate to `http://localhost:8888` in your browser and sign into JupyterLab. Once you start a distributed dask cluster, you will then have its port number (8787 by default if unoccupied).  \n",
    "\n",
    "Fortunately, you do not need to forward the Dask cluster port, as Jupyter can proxy it for you. You instead can use the following URL in your browser:\n",
    "```\n",
    "http://localhost:<jupyter_port>/proxy/<cluster_port>/status\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88672bdc-cc70-4f82-9b8a-bfad2a95abbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dbce92-e8db-4767-916a-8263ffec75df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
