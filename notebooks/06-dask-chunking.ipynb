{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "990cfa4c-2117-4435-9806-ff9048890398",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/NCAR/dask-tutorial/main/images/NCAR-contemp-logo-blue.png\"\n",
    "     width=\"750px\"\n",
    "     alt=\"NCAR logo\"\n",
    "     style=\"vertical-align:middle;margin:30px 0px\"/>\n",
    "\n",
    "# Dask Chunking - Best Practices\n",
    "\n",
    "**ESDS Dask tutorial | 06 February, 2023**  \n",
    "\n",
    "Negin Sobhani, Brian Vanderwende, Deepak Cherian, Ben Kirk  \n",
    "Computational & Information Systems Lab (CISL)  \n",
    "[negins@ucar.edu](mailto:negins@ucar.edu), [vanderwb@ucar.edu](mailto:vanderwb@ucar.edu)\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f1fc96-8189-48b4-a61c-c8f2d2fc1a58",
   "metadata": {},
   "source": [
    "### In this tutorial, you will learn:\n",
    "\n",
    "* Basic rules of thumb for chunking\n",
    "* The importance of conforming to file chunks\n",
    "* The impact of rechunking in the computational pipeline\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "* [Dask Chunking Documentation](https://docs.dask.org/en/stable/array-chunks.html)\n",
    "* [Choosing Chunk Sizes Blog Post](https://blog.dask.org/2021/11/02/choosing-dask-chunk-sizes)\n",
    "* [Xarray Chunking Documentation](https://docs.xarray.dev/en/stable/user-guide/dask.html#chunking-and-performance)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b54bf9-f29f-4aea-bc32-706e82472165",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chunking Considerations\n",
    "\n",
    "Determining the best approach for sizing your Dask chunks can be tricky and often requires intuition about both Dask and your particular dataset. There are various considerations you may need to account for depending on your workflow:\n",
    "\n",
    "* The size (in bytes) of your chunks vs your number of workers\n",
    "* The chunk layout of data read from disk (formats like HDF5, Zarr)\n",
    "* The access patterns of your computational pipeline\n",
    "\n",
    "**Dask Array with NumPy array chunks...**\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/stable/_images/dask-array.svg\" width=500px alt=\"Dask Array Chunks\">\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bad3462-c839-4d9c-916b-2268276a2440",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Starting up our PBS Cluster\n",
    "\n",
    "To demonstrate the affects of different chunking strategies, let's instantiate a `PBSCluster` with 4 workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317a0ce-f1b8-47c5-8790-a9fd266ca248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0ecda0-2777-4fcd-a1a8-f3cd462f4fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PBS cluster object\n",
    "cluster = PBSCluster(\n",
    "    job_name = 'dask-wk23-chunking',\n",
    "    cores = 1,\n",
    "    memory = '10GiB',\n",
    "    processes = 1,\n",
    "    local_directory = '/glade/scratch/vanderwb/temp/dask/spill/pbs.$PBS_JOBID',\n",
    "    resource_spec = 'select=1:ncpus=1:mem=10GB',\n",
    "    queue = 'casper',\n",
    "    walltime = '30:00',\n",
    "    interface = 'ib0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10475615-7181-4b46-b807-7e210d488c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity-check our setup\n",
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107629fa-34a3-426c-828e-bafe6a8d7ca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f23a4-a8f3-412e-b511-7b2f2c96c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a25dc4-9e04-4074-aa9e-21397b14c75b",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "\n",
    "## Chunk size - Load balancing vs. Overhead\n",
    "\n",
    "There is always an optimal chunk size given your hardware setup and computation problem that is neither too big nor too small. Finding this chunk size often requires some trial and error, but it is helpful to know what you are looking to avoid:\n",
    "\n",
    "* **Too small** - if your chunks are too small, you will end up spending a significant and wasteful amount of time waiting for Dask to perform overhead (scheduling tasks, data communication) relative to the time spent computing\n",
    "* **Too large** - you run the risk of spilling to dask or memory failures and the scheduler also has a more difficult time load balancing\n",
    "\n",
    "The following rules of thumb are known, but it will vary according to your workflow:\n",
    "\n",
    "|Too Small|Possibly Too Small|Optimal|Too Large|\n",
    "|-|-|-|-|\n",
    "|< 1 MB|1-100 MB|100 MB - 1 GB|> Spill threshold|\n",
    "\n",
    "In practice, using chunks close to 0.1-0.5 GB in size works well.\n",
    "\n",
    "#### Let's test these rules of thumb..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb9aa86-182b-4ca6-bf62-d8a7884b0306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spin up workers on our PBS cluster\n",
    "cluster.scale(4)\n",
    "client.wait_for_workers(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e389dd1-d6f7-46f4-8dc1-fae0543f9450",
   "metadata": {},
   "source": [
    "For this exercise, we will simply generate a random number **Dask Array** of sufficient size that it would not fit in our login session memory. Let's try different chunking strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efdf2dc-2d99-43ad-a0a6-a3aa71049e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d2aa49-94d0-4cd1-bc28-db3aef3815d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = da.random.random((60000, 72000), chunks = (30000,36000))\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c14713d-4374-463a-b0fe-394e43db5574",
   "metadata": {},
   "source": [
    "These chunks are too large. They will exceed our spill threshold (0.6-0.7) and even briefly exceed our pause limit (0.8). The only thing working in our favor in this configuration is that non-aggregation tasks should be well-balanced among the 4 workers with 4 chunks, and we have a short task graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47da0213-4c7e-483e-99a3-73393559c817",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = t.mean()\n",
    "task.dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74ce6b4-88d3-495e-8b54-66d69b68a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = task.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca6de5f-d0d3-4f28-914c-6f104859a6a1",
   "metadata": {},
   "source": [
    "In this next configuration, we end up specifying a configuration with very small chunks relative to the problem size. We will not come close to the memory limits, but we will incur significant overhead relative to our computational task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552aa90c-908d-42ca-bcad-bfad2e8022ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = da.random.random((60000, 72000), chunks = (1000,1000))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a139e9-ecf6-45d8-a3de-cf7d2561b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = t.mean()\n",
    "task.dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c72dedb-2ea7-4d8a-8c90-479a9492694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = task.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5001d3e2-468d-42ff-a71e-ecfdbe94385b",
   "metadata": {},
   "source": [
    "Next, we will choose chunk sizes that fall in our expected \"optimal\" range of `100 MiB - 1 GiB`. We should be allowing Dask to distribute work efficiently but not imposing a high overhead..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbedf51-7548-477d-bdda-b451b7be27c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = da.random.random((60000, 72000), chunks = (10000,6000))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1882a526-d8fd-4406-8bc4-4acc9b666739",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = t.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787282db-254c-4007-b015-63ac8e196b86",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Matching chunking in a netCDF4 file\n",
    "\n",
    "If you are using a chunked data format, it is best to specify Dask chunks which equal to or (better-yet) multiples of the chunk shape on disk. If chunk sizes aren't multiples of disk chunks, you risk unnecessary additional reads of data as multiple disk chunks will need to be read to populate each Dask chunk. This can be very inefficient!\n",
    "\n",
    "#### Inspecting file chunking\n",
    "\n",
    "The exact process for checking file chunking depends on the format. Using the netCDF4 Python module, we can query the chunking parameters of any variable in a netCDF4 file.\n",
    "\n",
    "*Classic netCDF files do not support chunking!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eedce9-d0df-4235-bcff-6152cfde17da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3daf33-2579-4861-85d5-b20360d4bbe5",
   "metadata": {},
   "source": [
    "We will use a data file from a model forecast dataset over the Arctic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bdef3e-6c2c-4007-b061-04aa85e052e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = '/glade/collections/rda/data/ds631.1/asr15.fcst3.3D/asr15km.fct.3D.20120916.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42027834-fa87-432b-bd5c-a10b88aa8eb9",
   "metadata": {},
   "source": [
    "Once we open the *dataset* (nc4 file), we can reference a variable of interest using a dictionary key and then get the dimensions of that variable using `get_dims()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442f91b0-a174-467c-9095-0bf0e67639fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_data = nc.Dataset(my_file)\n",
    "nc_data['CLDFRA'].get_dims()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897923a9-9ab3-4e7b-8362-9f84e0510832",
   "metadata": {},
   "source": [
    "We can then use the `chunking()` method to get our chunk size for each dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82965dea-3b73-492a-b013-1f1e363cb9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_data['CLDFRA'].chunking()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbed586-688c-4526-869e-f0267b6edf0b",
   "metadata": {},
   "source": [
    "### Specifying chunks using Xarray\n",
    "\n",
    "Now that we understand our file chunks, we can specify a preferred chunk size to `open_dataset`. Note that if we use the `chunks` parameter, any dimension we don't mention will be spanned in its entirety for chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ca1a2-3102-4f85-b44c-3a001a93e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1c6563-00f9-47ed-8e6c-1dcc5f7b0a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open dataset using chunking along Time dimension\n",
    "ds = xr.open_dataset(my_file, chunks = {'Time' : 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668891dd-5b6c-48bb-942a-90aee04c30a5",
   "metadata": {},
   "source": [
    "Since we are only specifying a chunk size for Time, this should be equivalent to the following chunk shape:\n",
    "```python\n",
    "chunks = {'Time' : 1,\n",
    "          'num_metgrid_levels' : -1,\n",
    "          'south_north' : -1,\n",
    "          'west_east' : -1 }\n",
    "```\n",
    "We can confirm that our chunks look as intended using the DataArray *repr*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a41e94-fafb-4d0e-9988-2569a1f9ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.CLDFRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea593bbf-aa22-41b0-97e3-e5ab68afe39f",
   "metadata": {},
   "source": [
    "**Note:** You can also retrieve the file chunk size from Xarray itself, but it is not shown in the above repr. Use the following DataArray (variable) attribute instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f541f89c-b2f8-49c7-aa93-70ef0bf8df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.CLDFRA.encoding[\"chunksizes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa65dda-efa9-4611-9e12-df5662d51ad8",
   "metadata": {},
   "source": [
    "Now let's benchmark various chunk configurations. Our initial guess achieves the recommended ratio of >= 2 chunks per worker, but does use multiples of the file chunk size except in the time dimension.\n",
    "\n",
    "For this benchmark, we will find the maximum cloud fraction across vertical levels at all locations and times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48125db1-00ac-4600-a786-7e6e6cb098b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = ds.CLDFRA.max(dim = \"num_metgrid_levels\").compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b68e87-834c-42c0-ad63-0c1f199f8fc9",
   "metadata": {},
   "source": [
    "Notice above that this file has chunking that does not divide evenly into the dimension sizes. We can specify that our chunks match the file chunks directly, but this will leave \"remainder\" chunks and will slightly increase overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e975b1ff-6c30-4270-ae2b-b945d96c02a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(my_file, chunks = {'Time' : 1, \"num_metgrid_levels\" : 16,\n",
    "                                        \"south_north\" : 355, \"east_west\" : 355})\n",
    "ds.CLDFRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9257791-d82e-4564-bfe4-909e9253af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = ds.CLDFRA.max(dim = \"num_metgrid_levels\").compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26a6004-aea9-4f99-b094-b0d8affa18fa",
   "metadata": {},
   "source": [
    "The most problematic case occurs when we have chunk sizes that are smaller than the file chunks in one or more dimensions. Let's evaluate the impact by using progressively smaller vertical level ranks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab203f6-20a9-4183-b95c-2e119b40f217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using half the file chunk size in the vertical (same number of chunks)\n",
    "ds = xr.open_dataset(my_file, chunks = {\"Time\" : 4, \"num_metgrid_levels\" : 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af94486-17c3-43a8-8bbf-e82cd99cd8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = ds.CLDFRA.max(dim = \"num_metgrid_levels\").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d0289-a14d-490d-a05e-5b43b3e80091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 1/4 the chunk size in the vertical\n",
    "ds = xr.open_dataset(my_file, chunks = {\"Time\" : 8, \"num_metgrid_levels\" : 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f33087-5468-48a0-a3c0-9dfbced66f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = ds.CLDFRA.max(dim = \"num_metgrid_levels\").compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4f9ecb-0608-4c79-b598-4d83b9509f0b",
   "metadata": {},
   "source": [
    "It is also possible to use \"auto\" chunking, whereby the DataArray chunks are calculated for you. Are these optimal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a204b9-95db-4516-a16e-56f30f3d98a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open dataset using auto-chunking\n",
    "ds = xr.open_dataset(my_file, chunks = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0857d634-18cc-4f1f-98fe-b18007dd9394",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.CLDFRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ad72d7-9b25-4516-8cbb-9c5068a881cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = ds.CLDFRA.max(dim = \"num_metgrid_levels\").compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e15589-c483-440d-ad96-f7fa98920515",
   "metadata": {},
   "source": [
    "**No! Avoid using auto chunking for files written in chunks!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23624a5-2174-4d73-aace-024646918580",
   "metadata": {},
   "source": [
    "## Rechunking is expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fafba9-b289-436d-aa72-3a66553e9539",
   "metadata": {},
   "source": [
    "There are various reasons Dask might need to rechunk data, but in any case, it can be an expensive operation with a large amount of communication required between workers.\n",
    "\n",
    "**Scenario:** We wish to get the mean difference between two versions of a model for the same case study. Unfortunately, while the grids match for each version, the file chunk size used was different.\n",
    "\n",
    "Here, we will emulate the scenario with Dask Arrays..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f62d6e-2c35-4298-a546-e570a6736f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_run = da.random.random((800,600,60,20), chunks = (400,300,30,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee642dae-b1c1-4aa2-873c-ab207be5fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682d897a-4158-49ae-8960-7eac3b67c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_run = da.random.random((800,600,60,20), chunks = (800,600,10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e045450-a2a8-49b3-9ada-e9af425ebae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00776a1b-0bf2-49fd-bfac-6b0e2d7a33a3",
   "metadata": {},
   "source": [
    "Let's set up and analyse (via a high-level task graph), the operations we will need to do to retrieve a mean-squared difference/error between our two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f773a71-37db-4023-ac13-81e7d19dba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean squared difference\n",
    "mse_graph = ((old_run - new_run) ** 2).sum() / old_run.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b005b366-dc3b-431b-b381-7dc26c79fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_graph.dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d799359-f2cb-40eb-ba06-d66eaef6b722",
   "metadata": {},
   "source": [
    "Note the two rechunking operations near the beginning of our task graph. Because our data arrays are chunked differently, Dask must rechunk first to avoid slowing down operations with large data transfers between workers. It is good that Dask does this, but rechunking is still expensive..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71844b87-19f7-4b9a-8ae3-54c8fafbea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mse_graph.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e618a-6772-4f7f-9498-a630dbbe6f5d",
   "metadata": {},
   "source": [
    "In most circumstances, we will want to rechunk this data ourselves manually, and then save state (probably by creating a new rechunked data file). This one-time cost means we will not need to rechunk again in the future.\n",
    "\n",
    "In our scenario, we would likely rechunk the old run data, since we expect all future runs will have the new chunking.\n",
    "\n",
    "```python\n",
    "old_run_rechunked = old_run.rechunk((800,600,10,1))\n",
    "```\n",
    "\n",
    "Once this is done in a conversion workflow, we could load the rechunked data in our current workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868a4114-6c60-4078-b6ea-45345702805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_run = da.random.random((800,600,60,20), chunks = (800,600,10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc6ff9e-0199-4bdf-90f9-90e3de1ea516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean squared difference\n",
    "mse_graph = ((old_run - new_run) ** 2).sum() / old_run.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2cbe54-fa8b-4850-893d-be2874adf7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_graph.dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86b1e64-3140-44f4-ade1-cd636216640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mse_graph.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be018325-ef95-424a-9bf2-24d33a26c640",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024686ad-6bfa-48ba-bd85-d005f9477902",
   "metadata": {},
   "source": [
    "## Takeaway Message\n",
    "\n",
    "Chunking is fundamental to Dask and its blocked-algorithm approach, so don't ignore intelligently sizing your data chunks. Finding the perfect chunk size is not the goal, but neglecting simple rules of thumb can lead to massive performance penalties when aggregated over a complex multipart analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL 2023a",
   "language": "python",
   "name": "npl-2023a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
