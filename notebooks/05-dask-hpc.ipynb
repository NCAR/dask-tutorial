{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "990cfa4c-2117-4435-9806-ff9048890398",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/NCAR/dask-tutorial/main/images/NCAR-contemp-logo-blue.png\"\n",
    "     width=\"750px\"\n",
    "     alt=\"NCAR logo\"\n",
    "     style=\"vertical-align:middle;margin:30px 0px\"/>\n",
    "\n",
    "# Dask on HPC - Starting Clusters, Monitoring, and Debugging\n",
    "\n",
    "**ESDS Dask tutorial | 06 February, 2023**  \n",
    "\n",
    "Negin Sobhani, Brian Vanderwende, Deepak Cherian, Ben Kirk  \n",
    "Computational & Information Systems Lab (CISL)  \n",
    "[negins@ucar.edu](mailto:negins@ucar.edu), [vanderwb@ucar.edu](mailto:vanderwb@ucar.edu)\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f1fc96-8189-48b4-a61c-c8f2d2fc1a58",
   "metadata": {},
   "source": [
    "### In this tutorial, you will learn:\n",
    "\n",
    "* How to configure and initialize an HPC Dask cluster via `dask-jobqueue`\n",
    "* How to manage and monitor the resource usage of your Dask workers\n",
    "* Understanding Dask worker logs\n",
    "* Controlling how and where data spills from memory to disk\n",
    "* Analyzing the impact of your Dask workflow on your allocation\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "* [dask-jobqueue documentation](https://jobqueue.dask.org/en/latest/)\n",
    "* [Diagnosing Distributed Dask Performance](https://distributed.dask.org/en/stable/diagnosing-performance.html)\n",
    "* [Dask HPC Configuratio Examples](https://jobqueue.dask.org/en/latest/configurations.html)\n",
    "* [Managing HPC allocations at NCAR](https://arc.ucar.edu/knowledge_base/70549817)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b54bf9-f29f-4aea-bc32-706e82472165",
   "metadata": {},
   "source": [
    "## Starting HPC Dask clusters with `dask-jobqueue`\n",
    "\n",
    "A defining feature of most HPC systems is the batch scheduler - *Slurm, PBS, LSF, etc...* These schedulers allow us to access the significant resources of the system and scale far beyond what is capable by a personal workstation.\n",
    "\n",
    "Using Dask on an HPC system is no different - we need to interact with the scheduler to provide Dask with ample compute resources. We *could* first start a job with multiple cores and a large amount of memory, and then use the **LocalCluster** to spawn workers. However, this approach only scales to a single node.\n",
    "\n",
    "The typical approach is to let Dask request resources directly from the job scheduler via a scheduler-specific cluster type. Such clusters are provided by the add-on `dask-jobqueue` package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8d2cd0-2836-4abb-ac00-79cfc2081cd6",
   "metadata": {},
   "source": [
    "### Creating a scheduled-cluster\n",
    "\n",
    "Since we use the PBS Pro scheduler at NCAR, we will use the **PBSCluster** Dask scheduler from `dask-jobqueue`. Initialization is similar to a **LocalCluster**, but with unique parameters specific to creating batch jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317a0ce-f1b8-47c5-8790-a9fd266ca248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36531de7-4dbe-442b-9d22-65c0370412d4",
   "metadata": {},
   "source": [
    "The parameters of the `PBSCluster` provide a basic template for the resources that will be assigned to each job..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0ecda0-2777-4fcd-a1a8-f3cd462f4fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PBS cluster object\n",
    "cluster = PBSCluster(\n",
    "    job_name = 'dask-wk23-hpc',\n",
    "    cores = 1,\n",
    "    memory = '4GiB',\n",
    "    processes = 1,\n",
    "    local_directory = '/local_scratch/pbs.$PBS_JOBID/dask/spill',\n",
    "    resource_spec = 'select=1:ncpus=1:mem=4GB',\n",
    "    queue = 'casper',\n",
    "    walltime = '30:00',\n",
    "    interface = 'ib0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfcb342-71f3-4def-b7f7-212e0c8ee395",
   "metadata": {},
   "source": [
    "*Since we are working on a shared system, you may get a port-in-use warning. This is no cause for alarm, but make sure you are not starting a duplicate cluster unintentionally.*\n",
    "\n",
    "We should pause and consider some of these settings...\n",
    "\n",
    "* The `cores` and `memory` parameters are used by Dask to define workers, while the `resource_spec` is used by PBS to define jobs. In this single-worker config, they should match!\n",
    "* PBS uses *GB* to mean 1024-based storage units. `dask-jobqueue` accurately calls these `GiB`.\n",
    "* We use `interface='ib0'` to instruct Dask to use TCP over Infiniband (the high-speed network on Casper), instead of the slower ethernet network.\n",
    "\n",
    "Note also that we are using one worker per PBS job. This is a reasonable default on Casper, but it is possible to group workers together on one or more PBS jobs as well by increasing the `cores` and `ncpus`. Here are some considerations:\n",
    "\n",
    "**Using less workers per job will:**\n",
    "* Increase job throughput on most systems (easier to backfill smaller jobs)\n",
    "* Will always avoid interpretor lock issues\n",
    "* Is conceptually easy to understand\n",
    "* May be more robust if system is unstable\n",
    "* Can speed up file-reads in some situations\n",
    "\n",
    "**Using more workers per job will:**\n",
    "* Less overhead in thread-friendly workflows\n",
    "* May allow for slightly higher memory thresholds since they will share a pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308fbaa0-e295-4d31-a9d0-13e8795a7c7d",
   "metadata": {},
   "source": [
    "#### It is good practice to check your validate your cluster before initiating any workers, by outputting the job script Dask will create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10475615-7181-4b46-b807-7e210d488c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb88ef-1c5c-4041-905b-31bd4fe09fed",
   "metadata": {},
   "source": [
    "*Note how some settings are showing up despite me not setting them... where does my account come from, for example?*\n",
    "\n",
    "Let's take a detour for a moment..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6098b8a8-5007-4618-a6a3-2b2dbf0ad4a6",
   "metadata": {},
   "source": [
    "### Dask configuration files\n",
    "\n",
    "We can customize the behavior of Dask using YAML-based configuration files. These have some advantages:\n",
    "\n",
    "* Eliminate user-specific configuration from your notebooks/scripts\n",
    "* Avoid repetition in defining clusters and other Dask objects\n",
    "* Potentially reduce errors from forgetting important settings\n",
    "\n",
    "And also some downsides:\n",
    "\n",
    "* Obfuscates settings from others (including your future self!)\n",
    "* Reduces portability and ease of debugging\n",
    "\n",
    "User configuration files are stored in `~/.config/dask` by default. System administrators may also provide default Dask configuration in `/etc/dask` or via the `DASK_ROOT_CONFIG` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a955a20-eeac-4ea9-8220-e7951fe18e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ~/.config/dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7599a5e1-ffab-4a8f-9998-92e7dfc933ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatically view configuration file(s) within Python\n",
    "from dask import config\n",
    "config.refresh()\n",
    "config.get('jobqueue.pbs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae2134d-ef6d-4d43-8a89-2e508b0ef281",
   "metadata": {},
   "source": [
    "### Live Performance Monitoring\n",
    "\n",
    "Using `dask.distributed` provides us with a powerful diagnostic tool you have already seen: the *Dashboard*. The Dashboard can be integrated into your Jupyter environment in two ways - either with a separate website accessible from the Client widget, or as tabs in your JupyterLab interface via the `dask-labextension` add-on.\n",
    "\n",
    "**Tip:** JupyterLab Dashboard tabs can be saved as a \"workspace\" and loaded back in future sessions.\n",
    "\n",
    "Let's see how both can be used to monitor Dask workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394abe8-d31b-454d-b3eb-dbf8ae16ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the client to load the Dashboard\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a153c2-129c-49fd-ba77-7d13fa5048c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the client repr\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882958c7-fae6-45c5-a6bf-a25b80a4b98c",
   "metadata": {},
   "source": [
    "The Dashboard is immediately accessible above when using NCAR's JupyterHub. This URL can also be entered into the Dashboard extension (click the Dask logo on the left toolbar), which allows you to add useful screens like `Task Stream` and `Workers Memory` to your Lab interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7cc604-dfe2-4d7e-b305-3d71ee64428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the cluster to 2 workers (which will use 2 jobs here)\n",
    "cluster.scale(2)\n",
    "\n",
    "# Block progress until workers have spawned (typically only in demos and benchmarks!)\n",
    "client.wait_for_workers(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a45c3b5-aad4-45b1-978b-a2374e83f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the workers from the cluster object\n",
    "cluster.workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a6709-858a-4434-9403-56ffe7a04f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the workers in the job scheduler\n",
    "!qstat -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba8a3f9-98e8-4260-8b7e-4b78e3f5a378",
   "metadata": {},
   "source": [
    "*As soon as we scale the cluster up, the clock is ticking on these PBS jobs. Be mindful of idle workers when using a batch scheduler!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b84d057-b1b4-48d4-b253-62097086fcc2",
   "metadata": {},
   "source": [
    "#### Dashboard demo: multi-file Xarray data analysis\n",
    "\n",
    "To demonstrate how the Dashboard can be useful, let's do some simple analysis of data files using Xarray. Here we load 19 days of GOES5 data, and compute the mean near-surface temperature across the western US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62fe240-c8bc-4857-9e03-7af100baa2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8edc52-70ad-4a20-ad41-3596838f621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a multi-file import and load data in parallel\n",
    "ds = xr.open_mfdataset(\"/glade/collections/rda/data/ds313.0/orig_res/2022/GEOS5_orig_res_202201[0-1]*.nc\", parallel = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf1b6f7-3290-4ffb-a982-c5e209e482b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the total size of the variable (this has not been read in yet!)\n",
    "print(\"Size of Variable = {:5.2f} GiB\".format(ds.T.nbytes / 1024 ** 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73db672-0e3e-4ad4-b09a-a762b1977901",
   "metadata": {},
   "source": [
    "This data is much too big for our worker template, but as we have seen the chunks will be smaller in size. We can see if they will fit in RAM or cause spill by querying the data array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a4042b-a3a2-4731-9ebb-33011337fecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The graphical repr of one DaskArray - T\n",
    "ds.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37816544-50c9-42b2-835c-ce24a650f3c5",
   "metadata": {},
   "source": [
    "It looks like our data chunks will fit into RAM, but we can verify using the Dashboard. Let's construct our computation. Here we do the following:\n",
    "1. Subset the \"western US\" from the data via lat/lon slices\n",
    "2. Take the mean of temperature values across our western US box\n",
    "3. Select the near-surface level (0)\n",
    "\n",
    "Remember, we are just creating the task graph here. No work will occur yet in our cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d37e389-3f74-4798-ae85-0a94f9b5c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our task graph\n",
    "sfc_mean_graph = ds.T.sel(lon = slice(235, 255), lat = slice(30,50)).mean([\"lat\",\"lon\"]).isel(lev = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee71e697-92c0-4832-9395-0864f3c7fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip - double-click the figure to see the actual size\n",
    "dask.visualize(sfc_mean_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93316148-6748-42b0-a1c1-c3f0d9e2966b",
   "metadata": {},
   "source": [
    "Now, we can use `.compute()` to start the computation on our cluster. Keep an eye on the dashboard plots to follow progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b746a91-75cc-4173-9cc1-9a6d1afd6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = sfc_mean_graph.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab77998c-2fd0-4672-be8c-8fedd11660a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e3e88-d0b7-4d36-a2f0-766382c6d819",
   "metadata": {},
   "source": [
    "Now, let's see what speedup we can get by manually scaling up our computation by 2x. This is not possible (*beyond a certain hardware limit*) on a `LocalCluster`, but is easy to do using `dask-jobqueue`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c28c82e-e191-42ca-85ba-4a911a4ca50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the cluster to 4 workers\n",
    "cluster.scale(4)\n",
    "client.wait_for_workers(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221d503-9bcd-4cd7-9999-8449bcf556b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does this look in PBS?\n",
    "!qstat -u vanderwb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f3f86d-1a98-400f-bb9f-16739b7ad666",
   "metadata": {},
   "source": [
    "One downside of scaling is that you end up with a worker pool that has different amounts of wallclock time remaining. The flow of your script across time is something to consider - if you need to spin up more workers after some collection of them has used significant walltime, it may make sense to first scale down your cluster to zero (or run `client.restart()`) and then instantiate the new workers.\n",
    "\n",
    "Here, we will also demonstrate another type of performance monitoring provided by `dask.distributed` - the *performance report*. Using a context manager, we can profile the computational components in the task stream and store it as an HTML file for future analysis. It provides a hard copy analysis of the computation, though unfortunately it does not record worker memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abd4e9-4294-4eb9-a5e4-2f3d85a2f7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try generating a \"performance report\" this time\n",
    "from dask.distributed import performance_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4d4d5a-a3f8-4f8d-a3af-fd033c51f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Since metrics are captured live anyway, the overhead from the report is small\n",
    "with performance_report(filename=\"dask-report.html\"):\n",
    "    result = sfc_mean_graph.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5793937e-5444-4c6c-af82-0c758076979c",
   "metadata": {},
   "source": [
    "Hopefully, we see a significant improvement in time-to-solution using 2x the workers.\n",
    "\n",
    "Another improvement we can make to our computation is to reduce the size of the problem down as much as possible before doing meaningful work. Let's try rearranging our graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a900c8-be09-4039-a95a-2617bcd463c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our improved task graph\n",
    "sfc_mean_graph = ds.T.sel(lon = slice(235, 255), lat = slice(30,50)).isel(lev = 0).mean([\"lat\",\"lon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf9403-0186-46bd-8cd5-c0d613be6162",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = sfc_mean_graph.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2412a835-f759-4af2-a34a-b488295e4909",
   "metadata": {},
   "source": [
    "Since flattening the level dimension will reduce data compared to the lat-lon box, let's do that first to make subsequent operations cheaper.\n",
    "\n",
    "|Indexing|Dimensions|Pts Eliminated|\n",
    "|-|-|-|\n",
    "|isel|721 x 1152 x 71|58,972,032|\n",
    "|sel|(721 - 81) x (1152 - 81) x 72|49,351,680|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2fd78d-e116-4e5b-95ad-a75e5e24b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we do better?\n",
    "sfc_mean_graph = ds.T.isel(lev = 0).sel(lon = slice(235, 255), lat = slice(30,50)).mean([\"lat\",\"lon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7224a727-6873-4988-afcf-b4bf40182015",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = sfc_mean_graph.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e065c-c2b1-43cb-9c06-65b3098ee881",
   "metadata": {},
   "source": [
    "In this case, optimizations to the base operation can yield better speed improvements than doubling the dask worker count. **Optimize your workflow first, if possible - then parallelize with Dask if still necessary.**\n",
    "\n",
    "Let's plot our results again to inspect for differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d727024-d802-46b0-8138-a08e6f118433",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24fb2f4-b5fb-46c3-9ac4-6df61e4baf1f",
   "metadata": {},
   "source": [
    "#### Aside: Clusters can also adaptively scale\n",
    "\n",
    "For interactive, exploratory work, *[adaptive scaling](https://docs.dask.org/en/stable/how-to/adaptive.html)* can be useful (also very useful on cloud platforms). This allows the cluster to dynamically scale up and down based on the (Dask) scheduler's estimation of resource needs. This capability is highly customizable, but one basic method would be to set bounds on the number of worker jobs that can be used:\n",
    "\n",
    "```\n",
    "cluster.adapt(minimum=0, maximum=12)\n",
    "```\n",
    "\n",
    "Another benefit of adaptive scaling is that you can use the worker `--lifetime` argument to tell Dask to cleanly end work on a worker and restart the PBS job. If you stagger the start of your workers, Dask will be able to shuffle tasks appropriately to produce a so-called *[infinite workload](https://jobqueue.dask.org/en/latest/advanced-tips-and-tricks.html#how-to-handle-job-queueing-system-walltime-killing-workers)*.\n",
    "\n",
    "On busy systems, adaptive scaling can slow down bursty computations because of queue waits between scale-down and scale-up cycles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73976d-d2e5-45fe-a29b-06346b9373b7",
   "metadata": {},
   "source": [
    "#### Optimization: Persisting data in worker memory\n",
    "\n",
    "Sometimes you will need to compute multiple parameters on data from Dask objects. Using `.persist()` to store intermediate data in worker memory can save computational time if used appropriately. The raw data can be persisted too, of course, but watch out for exhausting worker memory.\n",
    "\n",
    "Here we compare the time it takes - with and without persisting intermediate results - to compute our level-0 mean, a level-10 mean, and a mean across all model levels.\n",
    "\n",
    "We will also introduce another diagnostic tool here, the `MemorySampler` context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767453d0-148b-4281-b767-5831569868f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed.diagnostics import MemorySampler\n",
    "ms = MemorySampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228524e0-3ffa-4740-a4a1-eaf9fc101c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Without persistance\n",
    "with ms.sample(\"Original\"):\n",
    "    r1 = ds.T.isel(lev = 0).sel(lon = slice(235, 255), lat = slice(30,50)).mean([\"lat\",\"lon\"]).compute()\n",
    "    r2 = ds.T.isel(lev = 10).sel(lon = slice(235, 255), lat = slice(30,50)).mean([\"lat\",\"lon\"]).compute()\n",
    "    ra = ds.T.sel(lon = slice(235, 255), lat = slice(30,50)).mean([\"lev\",\"lat\",\"lon\"]).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39882b19-e745-4b44-a2c4-d19ffa662aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# With persistance\n",
    "with ms.sample(\"Persist\"):\n",
    "    T_means = ds.T.sel(lon = slice(235, 255), lat = slice(30,50)).mean([\"lat\",\"lon\"]).persist()\n",
    "    r1 = T_means.isel(lev = 0).compute()\n",
    "    r2 = T_means.isel(lev = 10).compute()\n",
    "    ra = T_means.mean(\"lev\").compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0a952c-02c1-4db3-8703-e66b39886299",
   "metadata": {},
   "source": [
    "Without persisting the intermediate results, Dask will only store r1 and r2 in worker memory, and so the indexing operations must be done from scratch each time.\n",
    "\n",
    "Let's look at the memory usage..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4966cc57-42f6-4f81-9386-dc5c5dbe3043",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.plot(align=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b47939d-f4a8-4c3f-9e8c-4d4e47e01673",
   "metadata": {},
   "source": [
    "Because the intermediate results are required for the second calculation in the original case, we do not even use more memory. A clear win!\n",
    "\n",
    "Of course, when persisting data it is extra important to clean up. Running `del` on your persisted client variable will clear those data from worker memory (*as long as they are not referenced by other variables*). The **progress** dashboard is a useful reminder that we have data persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eb05e6-f35e-473b-bed8-55e625d8b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del T_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b4fb0-ed7e-48be-a8f4-550e5edbc415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close current workers\n",
    "cluster.scale(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f503bce5-6ec0-4c4d-9ace-a75e92624768",
   "metadata": {},
   "source": [
    "### Debugging workers case study: memory and spill-to-disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a441ec-03b5-4f38-92a4-3b5028c96f65",
   "metadata": {},
   "source": [
    "In this section, we will demonstrate two common considerations when using Dask on HPC:\n",
    "* Dask data spilling to disk\n",
    "* Interacting with `dask.distributed` worker logs\n",
    "\n",
    "For this case study, we will generate progressively larger Dask arrays that eventually trigger memory conditions. Dask workers handle data in different ways in the following memory regimes:\n",
    "\n",
    "|Suggested Threshold|Case Study Value|Worker Behavior|\n",
    "|-|-|-|\n",
    "|0.6 (managed mem)|2.4 GB|Data is allocated on disk (spill)|\n",
    "|0.7 (process mem)|2.8 GB|Data is allocated on disk (spill)|\n",
    "|0.8|3.2 GB|New data allocation is paused|\n",
    "|0.95|3.8 GB|Worker is killed to avoid OOM|\n",
    "\n",
    "These thresholds can be set at cluster creation time or overridden by your Dask Distributed configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a719a-6e10-4aa5-a267-9a320616643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from distributed.worker import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e07b29d-2d9f-4a2a-93f9-6422df47b459",
   "metadata": {},
   "source": [
    "It is possible to write directly to worker logs (PBS job logs in our case) using the worker `logger` from Dask Distributed. Here, we define a function to call the logger on each worker, which we will run eagerly via `client.run`.\n",
    "\n",
    "Keep an eye on the worker memory Dashboard panel as our for loop proceeds..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a817f6-6a9e-440d-b118-39ff99385cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_message(chunk_size):\n",
    "    logger.info(\"Current chunk size = {} MiB\".format(chunk_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eb9129-8105-4954-b00c-69c822f92bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start up 4 new workers\n",
    "cluster.scale(4)\n",
    "client.wait_for_workers(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f1ea0c-63a1-4e6a-9c20-819592047235",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk_mib in [1600, 2400, 3200, 3900]:\n",
    "    client.run(log_message, chunk_mib)\n",
    "    chunk_size = chunk_mib / 8 * 1024 * 1024\n",
    "    print(\"Mean of {} MiB random array = {:0.2f}\".format(chunk_mib, da.random.random((chunk_size * 4), chunks=(chunk_size)).mean().compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456149fc-3fe5-482b-83bb-8d0ab5f11673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the most recent 4 worker logs - these should be our logs\n",
    "!ls -lrt dask-worker-logs | tail -n 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2090673f-d867-4b82-94d7-e43c527f43c5",
   "metadata": {},
   "source": [
    "We can open the log file in the Lab interface or a terminal and investigate the reason for the `KilledWorker` exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6e1e52-9078-4bf5-8c03-edabbfbb7629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the worker state in PBS after the failure\n",
    "!qstat -u vanderwb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35336aa6-3e52-48e1-957f-7e9e2fdfd0e2",
   "metadata": {},
   "source": [
    "Notice how the workers retried the final computation a few times before giving up. This behavior occurs when using the `nanny`, which attempts to restore workers when they are killed for various exceptions. (*the nanny is an additional lightweight process that monitors the health of the worker and enables functionality like full cluster restart*)\n",
    "\n",
    "Recall the aforementined memory thresholds - workers get killed by Dask before exceeding the absolute limit of a job (which if hit on certain systems, could kill the PBS job too). Because of this safety mechanism, our PBS jobs are still intact.\n",
    "\n",
    "If you are running a long computation that, *if restarted*, could exhaust the worker job's walltime, you can disable the nanny functionality and make exceptions fatal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0635f9-8b2c-44b4-86f7-0d35b6765242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down our client (thus terminating workers)\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7114f8e-bdd0-4d15-9802-f723981bf72d",
   "metadata": {},
   "source": [
    "## Analyzing your allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a322b0-c521-4424-9d0c-440d35f1fb45",
   "metadata": {},
   "source": [
    "Dask does not provide integrated tools for analyzing the impact to your allocation (though you could back out values with care). This job is best suited for the scheduler itself, assuming that you've carefully instantiated your workers.\n",
    "\n",
    "Consider this value from our cluster config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e848ef4b-a28d-472a-98a0-27f75254e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d903623-91fc-4e45-8a02-dd629f5028dc",
   "metadata": {},
   "source": [
    "This means that every Dask worker I start via this workflow's `PBSCluster` will have the job name **dask-wk23-hpc**. We can leverage this along with our `qhist` utility to query the logs:\n",
    "\n",
    "```bash\n",
    "qhist -u $USER -N dask-wk23-hpc -f numcpus,elapsed -c > qhist-dask.out\n",
    "```\n",
    "\n",
    "This command will query all dask-workers in today's scheduler logs and output to CSV, which we redirect to a file. The `name` field is a powerful tool. If instead you use a worker name specific to your script, you can easily query only the jobs from that script. *Confusingly, if you wish to set the job name when creating your cluster object, use the `job_name` parameter, not `name`!*\n",
    "\n",
    "We can then read in the CSV using pandas (or even using a Dask Dataframe!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2b5bd8-96d7-4180-b1b7-f491cab24804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c5d64-0567-464e-87af-63840f878ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dj = pd.read_csv(\"../data/qhist-dask.out\")\n",
    "dj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d766238b-6d3a-4aa3-85c5-71802d86a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Core-hours used by this notebook: {:.2f}\".format(sum(dj['NCPUs'] * dj['Elapsed (h)'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fbea5d-a01d-46b4-bcba-9e07b91b6a2d",
   "metadata": {},
   "source": [
    "Records from `qhist` span the time that PBS was used on each system, so with a bit of forward-thinking prep work (picking descriptive worker names), you can easily trace back usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b767a0a5-1622-419f-9088-2eeeb18d3f11",
   "metadata": {},
   "source": [
    "## Additional Considerations\n",
    "\n",
    "As we've shown, Dask is flexible and highly configurable. While we will not cover the following topics in depth, we encourage you to explore further on your own (and let us know in the *survey* if you would like learn more!).\n",
    "\n",
    "#### Dask Worker Count\n",
    "\n",
    "You may be wondering how to choose the number of workers. This question is tricky and can often depend on the state of the machine at any time. Here are some absolutes:\n",
    "\n",
    "1. Use more than a single worker unless debugging or profiling\n",
    "2. Do not use more workers than you have chunks - they will be idle\n",
    "\n",
    "And here are some guidelines:\n",
    "\n",
    "1. If you have to choose between more workers vs. more memory per worker, let the chunk size be your guide (more on this in the next notebook)\n",
    "2. In general, requesting less workers with more memory will take longer to get through the queue than more workers with less memory; *typically memory is more constrained than CPU cores on analysis machines*\n",
    "3. Using [**adaptive scaling**](https://docs.dask.org/en/stable/how-to/adaptive.html) will make your workflow throughput less senstive to the state of the HPC jobs queue\n",
    "\n",
    "#### Using Dask on GPUs\n",
    "\n",
    "Much like Xarray can use NumPy arrays or Dask arrays, Dask itself can use NumPy arrays or [CuPy](https://cupy.dev/) arrays - the latter of which are GPU enabled on both NVIDIA and AMD hardware. For NVIDIA users, the [RAPIDS](https://rapids.ai/) suite offers cuDF - a drop in replacement for pandas DataFrames which can also be used with Dask. And efforts are underway to effectively use GPUs with Xarray and Dask.\n",
    "\n",
    "For a starting point, check out the [Dask with GPUs](https://www2.cisl.ucar.edu/events/gpu-series-multiple-gpus-python-dask) tutorial CISL offered in Summer 2022."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL 2023a",
   "language": "python",
   "name": "npl-2023a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
